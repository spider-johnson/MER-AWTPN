
# Adaptive Weighted Temporal Prototype Network (AWTPN)

Official PyTorch implementation for the paper:

> **Adaptive Weighted Temporal Prototype Network for Multimodal Emotion Recognition**  
> (Submitted to *Information Processing & Management*, 2025)

---

## ğŸ§  Model Overview
AWTPN consists of three components tightly matched to the paper:
- **Shared Emotional Space (SES):** lightweight linear projections mapping audio and video features into a unified space.
- **Temporal Prototypical Network (TPN):** a learnable prototype table; distances from each audio segment / video frame to category prototypes (Eq. 6â€“8) provide alignment signals.
- **Adaptive Weight Module (AWM):** frame/segment-wise attention that converts distances to reliability weights and fuses modalities (Eq. 15â€“21).

Losses implemented (Eq. 9â€“12, 23â€“25):
- Intra-/Inter-class metric losses, Align loss, Contrastive loss, plus CE and MSE; combined via Î»/Î· weights.

---

## ğŸ”§ Requirements

- Python 3.9+
- PyTorch 2.0+

```bash
pip install -r requirements.txt
```

---

## ğŸ’¾ Datasets

We evaluate on three public datasets used in the paper:
- IEMOCAP
- MELD
- CMU-MOSEI

**We do not redistribute datasets.** See `data/README.md` for download instructions and expected folder layout.  
Preprocessing scripts are provided in `data/preprocess.py` and will cache processed tensors under `data/processed/`.

---

## ğŸš€ Quickstart

1) Edit a config file under `config/`, e.g. `config/iemocap.yaml`.  
2) Train:
```bash
python train.py --config config/iemocap.yaml
```
3) Evaluate:
```bash
python test.py --config config/iemocap.yaml --checkpoint checkpoints/awtpn_best.pth
```

> Minimal example uses dummy feature loaders if raw datasets are not present (to sanity-check the pipeline).

---

## ğŸ“Š Expected Results (from the paper)

| Dataset    | w-ACC | w-F1  |
|------------|------:|------:|
| IEMOCAP    | 77.21 | 78.25 |
| MELD       | 73.18 | 70.42 |
| CMU-MOSEI  | 61.09 | 61.29 |

(Exact numbers may vary with seeds, splits and encoder checkpoints.)

---

## ğŸ“ Repository Structure

```
AWTPN/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ train.py
â”œâ”€â”€ test.py
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ iemocap.yaml
â”‚   â”œâ”€â”€ meld.yaml
â”‚   â””â”€â”€ mosei.yaml
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ awtpn.py
â”‚   â”œâ”€â”€ ses_module.py
â”‚   â”œâ”€â”€ tpn_module.py
â”‚   â””â”€â”€ awm_module.py
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ losses.py
â”‚   â”œâ”€â”€ metrics.py
â”‚   â””â”€â”€ seed.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ README.md
â”‚   â””â”€â”€ preprocess.py
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_iemocap.sh
â”‚   â”œâ”€â”€ run_meld.sh
â”‚   â””â”€â”€ run_mosei.sh
â”œâ”€â”€ checkpoints/
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ results/
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ .gitignore
â””â”€â”€ LICENSE
```

---

## ğŸ” Reproducibility Notes

- We expose all Î»/Î· loss weights and (Î±_A, Î²_V) fusion weights in YAML configs.  
- `utils/seed.py` sets seeds for NumPy/PyTorch and enables deterministic CuDNN (optional).  
- We log metrics per-epoch and save the best checkpoint by validation w-F1.

---

## ğŸ“˜ Citation

```
@article{hao2025awtpn,
  title={Adaptive Weighted Temporal Prototype Network for Multimodal Emotion Recognition},
  author={Hao, Chenyu and collaborators},
  journal={Information Processing & Management},
  year={2025}
}
```

---

## ğŸªª License

MIT License Â© 2025 Chenyu Hao
